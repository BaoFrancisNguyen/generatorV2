#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ROUTES DE G√âN√âRATION - G√âN√âRATEUR MALAYSIA
Fichier: app/routes/generation.py

Routes pour la g√©n√©ration de donn√©es √©nerg√©tiques:
- G√©n√©ration standard avec param√®tres personnalis√©s
- G√©n√©ration bas√©e sur OpenStreetMap
- Aper√ßu et validation des param√®tres
- Export dans diff√©rents formats

Auteur: √âquipe D√©veloppement
Date: 2025
Version: 3.0 - Routes modulaires
"""

import logging
from datetime import datetime
from flask import Blueprint, request, jsonify, current_app, send_file
from werkzeug.exceptions import BadRequest

from app.utils.validators import validate_generation_request, validate_osm_request

# Cr√©er le blueprint pour les routes de g√©n√©ration
generation_bp = Blueprint('generation', __name__)
logger = logging.getLogger(__name__)


@generation_bp.route('/', methods=['POST'])
def generate_standard():
    """
    G√©n√©ration standard de donn√©es √©nerg√©tiques avec param√®tres personnalis√©s.
    
    Body JSON attendu:
    {
        "num_buildings": 100,
        "start_date": "2024-01-01",
        "end_date": "2024-01-31",
        "frequency": "D",
        "location_filter": {...},
        "building_types": [...],
        "export_format": "parquet"
    }
    
    Returns:
        JSON avec les donn√©es g√©n√©r√©es ou le chemin du fichier d'export
    """
    logger.info("üèóÔ∏è Demande de g√©n√©ration standard")
    
    try:
        # Validation de la requ√™te
        if not request.is_json:
            raise BadRequest("Content-Type application/json requis")
        
        data = request.get_json()
        if not data:
            raise BadRequest("Corps de requ√™te JSON vide")
        
        # Valider les param√®tres
        validation_result = validate_generation_request(data, current_app.config)
        if not validation_result['valid']:
            return jsonify({
                'success': False,
                'error': 'Param√®tres invalides',
                'details': validation_result['errors']
            }), 400
        
        # Extraire les param√®tres
        generation_params = {
            'num_buildings': data.get('num_buildings', current_app.config['DEFAULT_NUM_BUILDINGS']),
            'start_date': data.get('start_date', current_app.config['DEFAULT_START_DATE']),
            'end_date': data.get('end_date', current_app.config['DEFAULT_END_DATE']),
            'frequency': data.get('frequency', current_app.config['DEFAULT_FREQUENCY']),
            'location_filter': data.get('location_filter'),
            'building_types': data.get('building_types')
        }
        
        export_format = data.get('export_format', 'json')
        return_data = data.get('return_data', True)
        
        logger.info(f"üìä G√©n√©ration de {generation_params['num_buildings']} b√¢timents "
                   f"du {generation_params['start_date']} au {generation_params['end_date']}")
        
        # G√©n√©ration des donn√©es
        start_time = datetime.now()
        dataset = current_app.data_generator.generate_complete_dataset(**generation_params)
        generation_time = (datetime.now() - start_time).total_seconds()
        
        # Validation des donn√©es g√©n√©r√©es
        validation_results = current_app.validation_service.validate_complete_dataset(
            dataset['buildings'], 
            dataset['timeseries']
        )
        
        # Pr√©parer la r√©ponse
        response_data = {
            'success': True,
            'generation_time_seconds': round(generation_time, 2),
            'metadata': dataset['metadata'],
            'validation': validation_results,
            'statistics': _calculate_dataset_statistics(dataset)
        }
        
        # Retourner les donn√©es ou exporter selon le format demand√©
        if return_data and export_format == 'json':
            # Retourner directement les donn√©es en JSON
            response_data['data'] = {
                'buildings': dataset['buildings'].to_dict('records'),
                'timeseries': dataset['timeseries'].to_dict('records')
            }
            return jsonify(response_data)
        
        else:
            # Exporter vers un fichier
            export_result = current_app.export_service.export_dataset(
                dataset, 
                export_format,
                include_metadata=True
            )
            
            response_data['export'] = {
                'format': export_format,
                'files': export_result['files'],
                'total_size_bytes': export_result['total_size']
            }
            
            if data.get('download_immediately', False):
                # T√©l√©chargement imm√©diat du premier fichier
                return send_file(
                    export_result['files'][0]['path'],
                    as_attachment=True,
                    download_name=export_result['files'][0]['filename']
                )
            
            return jsonify(response_data)
        
    except BadRequest as e:
        logger.warning(f"‚ö†Ô∏è Requ√™te invalide: {e}")
        return jsonify({
            'success': False,
            'error': 'Requ√™te invalide',
            'details': str(e)
        }), 400
    
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la g√©n√©ration standard: {e}")
        return jsonify({
            'success': False,
            'error': 'Erreur lors de la g√©n√©ration',
            'details': str(e)
        }), 500


@generation_bp.route('/osm', methods=['POST'])
def generate_from_osm():
    """
    G√©n√©ration bas√©e sur des b√¢timents r√©els d'OpenStreetMap.
    
    Body JSON attendu:
    {
        "city": "Kuala Lumpur",
        "bbox": [south, west, north, east],  // Alternative √† city
        "building_types": ["residential", "commercial"],
        "limit": 1000,
        "start_date": "2024-01-01",
        "end_date": "2024-01-31",
        "frequency": "D",
        "export_format": "parquet"
    }
    
    Returns:
        JSON avec les donn√©es g√©n√©r√©es bas√©es sur OSM
    """
    logger.info("üó∫Ô∏è Demande de g√©n√©ration bas√©e sur OSM")
    
    try:
        # Validation de la requ√™te
        if not request.is_json:
            raise BadRequest("Content-Type application/json requis")
        
        data = request.get_json()
        if not data:
            raise BadRequest("Corps de requ√™te JSON vide")
        
        # Valider les param√®tres OSM
        validation_result = validate_osm_request(data, current_app.config)
        if not validation_result['valid']:
            return jsonify({
                'success': False,
                'error': 'Param√®tres OSM invalides',
                'details': validation_result['errors']
            }), 400
        
        # R√©cup√©rer les b√¢timents OSM
        start_time = datetime.now()
        
        if 'city' in data:
            # R√©cup√©ration par ville
            osm_buildings = current_app.osm_service.get_buildings_for_city(
                city_name=data['city'],
                limit=data.get('limit')
            )
        elif 'bbox' in data:
            # R√©cup√©ration par bbox
            osm_buildings = current_app.osm_service.get_buildings_in_bbox(
                bbox=data['bbox'],
                building_types=data.get('building_types')
            )
        elif 'coordinates' in data:
            # R√©cup√©ration autour d'un point
            coords = data['coordinates']
            osm_buildings = current_app.osm_service.get_buildings_around_point(
                lat=coords['lat'],
                lon=coords['lon'],
                radius=data.get('radius', 1000),
                building_types=data.get('building_types')
            )
        else:
            raise BadRequest("Param√®tre de localisation requis (city, bbox, ou coordinates)")
        
        osm_fetch_time = (datetime.now() - start_time).total_seconds()
        
        if not osm_buildings:
            return jsonify({
                'success': False,
                'error': 'Aucun b√¢timent trouv√©',
                'details': 'Aucun b√¢timent OSM trouv√© pour les crit√®res sp√©cifi√©s'
            }), 404
        
        logger.info(f"üìç {len(osm_buildings)} b√¢timents OSM r√©cup√©r√©s en {osm_fetch_time:.2f}s")
        
        # Convertir les b√¢timents OSM en format g√©n√©rateur
        buildings_df = _convert_osm_to_generator_format(osm_buildings)
        
        # G√©n√©ration des s√©ries temporelles
        generation_start = datetime.now()
        timeseries_df = current_app.data_generator.generate_timeseries_data(
            buildings_df=buildings_df,
            start_date=data.get('start_date', current_app.config['DEFAULT_START_DATE']),
            end_date=data.get('end_date', current_app.config['DEFAULT_END_DATE']),
            frequency=data.get('frequency', current_app.config['DEFAULT_FREQUENCY'])
        )
        generation_time = (datetime.now() - generation_start).total_seconds()
        
        # Cr√©er le dataset complet
        dataset = {
            'buildings': buildings_df,
            'timeseries': timeseries_df,
            'metadata': {
                'osm_source': True,
                'osm_fetch_time_seconds': round(osm_fetch_time, 2),
                'generation_time_seconds': round(generation_time, 2),
                'total_buildings': len(buildings_df),
                'total_observations': len(timeseries_df),
                'osm_query_info': {
                    'city': data.get('city'),
                    'bbox': data.get('bbox'),
                    'building_types_filter': data.get('building_types'),
                    'limit': data.get('limit')
                }
            }
        }
        
        # Validation des donn√©es
        validation_results = current_app.validation_service.validate_complete_dataset(
            buildings_df, timeseries_df
        )
        
        # Pr√©parer la r√©ponse
        response_data = {
            'success': True,
            'osm_buildings_found': len(osm_buildings),
            'osm_fetch_time_seconds': round(osm_fetch_time, 2),
            'generation_time_seconds': round(generation_time, 2),
            'total_time_seconds': round(osm_fetch_time + generation_time, 2),
            'metadata': dataset['metadata'],
            'validation': validation_results,
            'statistics': _calculate_dataset_statistics(dataset)
        }
        
        # Export ou retour des donn√©es
        export_format = data.get('export_format', 'json')
        
        if data.get('return_data', True) and export_format == 'json':
            response_data['data'] = {
                'buildings': buildings_df.to_dict('records'),
                'timeseries': timeseries_df.to_dict('records')
            }
            return jsonify(response_data)
        else:
            # Export vers fichier
            export_result = current_app.export_service.export_dataset(
                dataset, 
                export_format,
                include_metadata=True,
                filename_prefix='osm_generated'
            )
            
            response_data['export'] = {
                'format': export_format,
                'files': export_result['files'],
                'total_size_bytes': export_result['total_size']
            }
            
            return jsonify(response_data)
        
    except BadRequest as e:
        logger.warning(f"‚ö†Ô∏è Requ√™te OSM invalide: {e}")
        return jsonify({
            'success': False,
            'error': 'Requ√™te invalide',
            'details': str(e)
        }), 400
    
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la g√©n√©ration OSM: {e}")
        return jsonify({
            'success': False,
            'error': 'Erreur lors de la g√©n√©ration OSM',
            'details': str(e)
        }), 500


@generation_bp.route('/preview', methods=['POST'])
def preview_generation():
    """
    Aper√ßu des param√®tres de g√©n√©ration avec estimations.
    
    Returns:
        JSON avec les estimations de temps, taille, et caract√©ristiques
    """
    logger.info("üëÅÔ∏è Demande d'aper√ßu de g√©n√©ration")
    
    try:
        data = request.get_json() or {}
        
        # Param√®tres par d√©faut
        num_buildings = data.get('num_buildings', current_app.config['DEFAULT_NUM_BUILDINGS'])
        start_date = data.get('start_date', current_app.config['DEFAULT_START_DATE'])
        end_date = data.get('end_date', current_app.config['DEFAULT_END_DATE'])
        frequency = data.get('frequency', current_app.config['DEFAULT_FREQUENCY'])
        
        # Calculs d'estimation
        estimations = _calculate_generation_estimations(
            num_buildings, start_date, end_date, frequency
        )
        
        # Recommandations
        recommendations = _get_generation_recommendations(estimations)
        
        preview_data = {
            'success': True,
            'parameters': {
                'num_buildings': num_buildings,
                'start_date': start_date,
                'end_date': end_date,
                'frequency': frequency,
                'location_filter': data.get('location_filter'),
                'building_types': data.get('building_types')
            },
            'estimations': estimations,
            'recommendations': recommendations,
            'warnings': _get_generation_warnings(estimations),
            'compatibility': {
                'formats_supported': current_app.config.get('SUPPORTED_EXPORT_FORMATS', []),
                'max_buildings': current_app.config.get('MAX_BUILDINGS'),
                'max_period_days': current_app.config.get('MAX_PERIOD_DAYS')
            }
        }
        
        return jsonify(preview_data)
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'aper√ßu: {e}")
        return jsonify({
            'success': False,
            'error': 'Erreur lors de l\'aper√ßu',
            'details': str(e)
        }), 500


@generation_bp.route('/sample', methods=['GET'])
def generate_sample():
    """
    G√©n√®re un √©chantillon de donn√©es pour d√©monstration.
    
    Returns:
        JSON avec un petit dataset d'exemple
    """
    logger.info("üß™ G√©n√©ration d'√©chantillon de d√©monstration")
    
    try:
        # Param√®tres fixes pour l'√©chantillon
        sample_params = {
            'num_buildings': 5,
            'start_date': '2024-01-01',
            'end_date': '2024-01-07',  # 1 semaine
            'frequency': 'H'  # Donn√©es horaires
        }
        
        # G√©n√©ration rapide
        start_time = datetime.now()
        dataset = current_app.data_generator.generate_complete_dataset(**sample_params)
        generation_time = (datetime.now() - start_time).total_seconds()
        
        sample_data = {
            'success': True,
            'sample_type': 'demonstration',
            'generation_time_seconds': round(generation_time, 2),
            'metadata': dataset['metadata'],
            'data': {
                'buildings': dataset['buildings'].to_dict('records'),
                'timeseries': dataset['timeseries'].head(50).to_dict('records')  # Limiter √† 50 observations
            },
            'statistics': _calculate_dataset_statistics(dataset),
            'note': 'Ceci est un √©chantillon de d√©monstration avec des donn√©es limit√©es'
        }
        
        return jsonify(sample_data)
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la g√©n√©ration d'√©chantillon: {e}")
        return jsonify({
            'success': False,
            'error': 'Erreur lors de la g√©n√©ration d\'√©chantillon',
            'details': str(e)
        }), 500


# Fonctions utilitaires priv√©es

def _convert_osm_to_generator_format(osm_buildings):
    """
    Convertit les b√¢timents OSM au format attendu par le g√©n√©rateur.
    
    Args:
        osm_buildings: Liste des b√¢timents OSM
        
    Returns:
        DataFrame au format g√©n√©rateur
    """
    import pandas as pd
    
    converted_buildings = []
    
    for osm_building in osm_buildings:
        # G√©n√©rer un ID unique pour le g√©n√©rateur
        unique_id = current_app.data_generator._generate_unique_id()
        
        # Convertir au format g√©n√©rateur
        building_data = {
            'unique_id': unique_id,
            'building_id': f"OSM_{osm_building.get('osm_id', unique_id)}",
            'latitude': osm_building['latitude'],
            'longitude': osm_building['longitude'],
            'location': osm_building.get('city', 'Unknown'),
            'building_class': osm_building['building_type'],
            'osm_source': True,
            'osm_id': osm_building.get('osm_id'),
            'osm_tags': osm_building.get('tags', {}),
            'area_sqm': osm_building.get('area_sqm'),
            'levels': osm_building.get('levels'),
            'height': osm_building.get('height')
        }
        
        converted_buildings.append(building_data)
    
    return pd.DataFrame(converted_buildings)


def _calculate_dataset_statistics(dataset):
    """
    Calcule les statistiques d'un dataset g√©n√©r√©.
    
    Args:
        dataset: Dataset avec 'buildings' et 'timeseries'
        
    Returns:
        Dictionnaire des statistiques
    """
    buildings_df = dataset['buildings']
    timeseries_df = dataset['timeseries']
    
    # Statistiques de base
    stats = {
        'total_buildings': len(buildings_df),
        'total_observations': len(timeseries_df),
        'unique_locations': buildings_df['location'].nunique() if 'location' in buildings_df.columns else 0,
        'building_types': buildings_df['building_class'].value_counts().to_dict() if 'building_class' in buildings_df.columns else {},
        'consumption_stats': {}
    }
    
    # Statistiques de consommation
    if 'y' in timeseries_df.columns:
        consumption_col = 'y'
    elif 'consumption_kwh' in timeseries_df.columns:
        consumption_col = 'consumption_kwh'
    else:
        consumption_col = None
    
    if consumption_col:
        consumption_data = timeseries_df[consumption_col]
        stats['consumption_stats'] = {
            'mean_kwh': round(consumption_data.mean(), 2),
            'median_kwh': round(consumption_data.median(), 2),
            'max_kwh': round(consumption_data.max(), 2),
            'min_kwh': round(consumption_data.min(), 2),
            'std_kwh': round(consumption_data.std(), 2),
            'total_kwh': round(consumption_data.sum(), 2)
        }
    
    # P√©riode temporelle
    if 'timestamp' in timeseries_df.columns:
        timestamps = pd.to_datetime(timeseries_df['timestamp'])
        stats['temporal_coverage'] = {
            'start_date': timestamps.min().isoformat(),
            'end_date': timestamps.max().isoformat(),
            'total_days': (timestamps.max() - timestamps.min()).days + 1
        }
    
    return stats


def _calculate_generation_estimations(num_buildings, start_date, end_date, frequency):
    """
    Calcule les estimations pour une g√©n√©ration.
    
    Returns:
        Dictionnaire avec les estimations
    """
    from datetime import datetime
    import pandas as pd
    
    # Calculer le nombre d'observations
    date_range = pd.date_range(start=start_date, end=end_date, freq=frequency)
    total_observations = len(date_range) * num_buildings
    
    # Estimations de temps (bas√©es sur des benchmarks)
    base_time_per_building = 0.01  # 10ms par b√¢timent
    base_time_per_observation = 0.001  # 1ms par observation
    
    estimated_time = (num_buildings * base_time_per_building) + (total_observations * base_time_per_observation)
    
    # Estimation de taille (approximative)
    bytes_per_observation = 150  # Estimation bas√©e sur le format parquet
    estimated_size_bytes = total_observations * bytes_per_observation
    
    return {
        'total_observations': total_observations,
        'estimated_time_seconds': round(estimated_time, 2),
        'estimated_size_bytes': estimated_size_bytes,
        'estimated_size_mb': round(estimated_size_bytes / (1024 * 1024), 2),
        'complexity_level': _assess_complexity_level(num_buildings, total_observations)
    }


def _assess_complexity_level(num_buildings, total_observations):
    """√âvalue le niveau de complexit√© d'une g√©n√©ration."""
    if total_observations < 10000:
        return 'low'
    elif total_observations < 100000:
        return 'medium'
    elif total_observations < 1000000:
        return 'high'
    else:
        return 'very_high'


def _get_generation_recommendations(estimations):
    """G√©n√®re des recommandations bas√©es sur les estimations."""
    recommendations = []
    
    complexity = estimations['complexity_level']
    
    if complexity == 'low':
        recommendations.append("Configuration id√©ale pour les tests et le d√©veloppement")
    elif complexity == 'medium':
        recommendations.append("Configuration appropri√©e pour l'entra√Ænement de mod√®les")
    elif complexity == 'high':
        recommendations.append("Configuration pour l'analyse de production - temps de g√©n√©ration √©lev√©")
    else:
        recommendations.append("Configuration tr√®s lourde - consid√©rez r√©duire la p√©riode ou le nombre de b√¢timents")
    
    if estimations['estimated_time_seconds'] > 300:  # 5 minutes
        recommendations.append("Temps de g√©n√©ration √©lev√© - consid√©rez l'export direct en fichier")
    
    if estimations['estimated_size_mb'] > 100:  # 100MB
        recommendations.append("Dataset volumineux - format Parquet recommand√© pour l'efficacit√©")
    
    return recommendations


def _get_generation_warnings(estimations):
    """G√©n√®re des avertissements bas√©s sur les estimations."""
    warnings = []
    
    if estimations['estimated_time_seconds'] > 600:  # 10 minutes
        warnings.append("Temps de g√©n√©ration tr√®s √©lev√© (>10 minutes)")
    
    if estimations['estimated_size_mb'] > 500:  # 500MB
        warnings.append("Dataset tr√®s volumineux (>500MB)")
    
    if estimations['total_observations'] > 5000000:  # 5M observations
        warnings.append("Nombre d'observations tr√®s √©lev√© - peut impacter les performances")
    
    return warnings


# Export du blueprint
__all__ = ['generation_bp']
